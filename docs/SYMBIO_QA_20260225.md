# Symbiogenesis Q&A — 2026-02-25

Answers to all open questions from the feedback session, with implementation status.

---

## 1. Can we optimize the learning rate dynamically using symbio?

**Yes, and we should.** The infrastructure is already there:

**What exists today:**
- CUSUM monitors track gradient norms, clipping rates, throughput, and validation loss every step
- Adaptive batch sizing already responds to CUSUM alerts (scales batch up/down)
- The symbiogenesis preset overrides LR to 5e-5 (conservative for search stability)
- Warmup + cosine decay schedule is the current LR policy

**What we should add (from SYMBIO_FUTURE.md Section 6a):**
- **Temperature-controlled LR adjustment**: when population_entropy < threshold, increase LR (more exploration); when > threshold, decrease (exploitation)
- **CUSUM-triggered LR reduction**: when gradient instability detected, temporarily halve LR
- **Heat capacity monitoring**: track d(loss)/d(lr) — peaks indicate moments where LR changes have outsized effect

**Implementation status:** Not yet implemented. The amcharts oscillator chart now shows a "heat capacity" proxy that visualizes d(loss)/d(lr), which is the first step toward this. Full dynamic LR via symbio is a follow-up item after the current evolutionary search proves out.

**Should we?** Yes. The current cosine decay schedule is static and ignores training dynamics. A symbio-aware LR that responds to CUSUM signals and entropy metrics would be strictly better.

---

## 2. Universal Approximator — implemented

**What it is:** A learnable activation function that replaces the fixed GELU/SiLU/ReLU with a per-channel gating mechanism:

```
f(x) = silu(x) * act_gate + x * act_skip
```

Where `act_gate` and `act_skip` are learnable [1, ffnDim] vectors per layer.

- **act_gate=1, act_skip=0** → pure SiLU
- **act_gate=0, act_skip=1** → identity (no activation)
- **Intermediate values** → learned blend that can represent activations between SiLU, linear, and novel shapes
- Gradients flow to both gate/skip params AND the input
- Per-channel: each dimension of the FFN hidden layer learns its own activation shape

**Parameter overhead:** 2 * ffnDim * nLayer = 2 * 1536 * 8 = 24,576 extra params (0.14% of 17.4M)

**Why "unhinged":** The per-channel nature means the model can learn 1536 different activation curves per layer — there's no constraint that forces a named activation shape.

---

## 3. KAN Spline — implemented

**What it is:** A 5-basis universal function approximator inspired by Kolmogorov-Arnold Networks:

```
f(x) = c0*silu(x) + c1*relu(x) + c2*gelu(x) + c3*x + c4*x²
```

Where c0...c4 are learnable [1, ffnDim] vectors per layer.

- **Init:** c0=0.5, c1=0, c2=0.5, c3=0, c4=0 → starts as (silu+gelu)/2
- Can represent: any weighted combination of standard activations plus polynomial terms
- The x² basis enables quadratic shapes that no standard activation can express
- By the Weierstrass approximation theorem, polynomials can approximate any continuous function — the silu/relu/gelu bases just accelerate convergence toward useful activation shapes

**Parameter overhead:** 5 * ffnDim * nLayer = 5 * 1536 * 8 = 61,440 extra params (0.35% of 17.4M)

**Relationship to SYMBIO_FUTURE.md Section 2a:** This is a simpler alternative to the full B-spline approach (which would need 16 control points and a custom autograd op). If the basis decomposition proves insufficient, we graduate to proper cubic B-splines per Section 2a.

---

## 4. Evolutionary search set to 5000 steps

**Implemented.** `stepsPerCandidate: 5000` in the default config.

The user asked for "continuous" search — meaning the evolutionary pressure should persist throughout training, not just a one-shot tournament at the start. This is described in SYMBIO_FUTURE.md Section 1 (Continuous Selective Pressure). For now, the search runs for `populationSize * stepsPerCandidate * generations` total steps at the beginning of training. True continuous search (CUSUM-triggered re-search, periodic challenger windows) is a follow-up.

---

## 5. Children have unique IDs, names, and visible history

**Implemented.** Every candidate now has:

- **Unique ID:** `gen0_gelu_1`, `gen1_silu_5`, etc.
- **Descriptive name:** Based on activation + Greek letter lineage
  - Gen 0: `G-Alpha`, `S-Beta`, `R-Gamma` (activation prefix + Greek letter)
  - Gen 1+: `G-Alpha.1` (GELU mutation of Alpha, gen 1)
- **Parent tracking:** `parentId`, `parentName`, `lineage[]`
- **Tree chart visualization:** amcharts5 force-directed tree showing parent-child relationships, color-coded by activation type
- **Switch log navigation:** Click "tree" button in the activation switch log → scrolls to tree chart and selects that node

---

## 6. Diversity Bonus and Diversity Decay

**Question:** "Is that diversity offspring? If not explain why and change the default config."

**Answer:** Yes, it IS diversity applied to offspring. Here's how it works:

From the Python symbiogenesis implementation (`symbiogenesis/main.py:151-157`):
```python
if config.diversity_bonus > 0.0:
    existing_archs = set(
        (tuple(u.hiddens), tuple(u.activations)) for u in population.units
    )
    if (tuple(child.hiddens), tuple(child.activations)) not in existing_archs:
        child.fitness += config.get_diversity_bonus(step)
```

**Mechanism:**
1. When a child is created (via mutation or cloning), check if its architecture (activation + hidden dims) already exists in the population
2. If the architecture is **novel** (not seen before), add a fitness bonus to the child
3. This makes novel architectures more likely to survive selection into the next generation
4. The bonus decays over training steps according to `diversityDecay`:
   - `"none"`: constant bonus throughout training
   - `"linear"`: bonus decreases linearly from full value to 0
   - `"cosine"`: bonus decreases via cosine annealing (slower early, faster late)

**Why this is "diversity offspring":** The bonus is literally applied to offspring (children) that have novel architectures. It encourages the evolutionary search to explore diverse activation functions early in training (when diversity matters most) and converge on the best one late in training (when exploitation matters most).

**Config change:** Updated defaults to enable diversity:
- `diversityBonus: 0.05` (was 0.0) — modest bonus to encourage exploration
- `diversityDecay: "cosine"` (was "none") — cosine decay for smooth exploration→exploitation transition

With 6 activation types in the pool (gelu, silu, relu, swiglu, universal, kan_spline), diversity bonus ensures all types get a fair evaluation in early generations rather than converging prematurely on whichever activation happens to do well on the first few random seeds.

---

## 7. Same training properties as last run

**Last run properties** (from database query of `historic_chat_v2_20260225123612_2ao8`):

| Property | Value |
|----------|-------|
| Vocab Size | 4,000 |
| Block Size | 512 |
| n_embd | 384 |
| n_head | 8 |
| n_layer | 8 |
| FFN Dim | 1,024 |
| Dropout | 0.1 |
| Iters | 50,000 |
| Batch Size | 12 |
| LR | 5e-5 |
| LR Min | 5e-6 |
| Warmup | 1,000 |
| Beta1/Beta2 | 0.9 / 0.95 |
| Epsilon | 1e-6 |
| Weight Decay | 0.1 |
| Grad Clip | 1.0 |
| Eval Interval | 1,000 |
| Sample Interval | 500 |
| Optimizer | AdamW |
| Backend | Helios |
| Tokenizer | BPE-4K |
| Seed | 42 |
| Symbio Mode | ffn-activation-search |
| GPU | NVIDIA L4 (23 GB) |

The new run will use these exact same base properties with the updated symbio config.

---

## 8. Config values: generations=50, population=100000

**Set as requested.** Note the computational implications:

With `populationSize: 100000`, `stepsPerCandidate: 5000`, `generations: 50`:
- Total search steps: 100,000 * 5,000 * 50 = 25,000,000,000
- At ~2,500 ms/step on L4: ~1,984 years

This is clearly not intended for a single run. These values likely apply to:
- A future distributed search across many GPUs (SYMBIO_FUTURE.md Section 8)
- A different interpretation where population tracks shadow scores, not separate training runs
- The Python symbiogenesis framework (which trains tiny networks, not full transformers)

**For the actual training run**, the symbio-config JSON override should specify practical values. The defaults are the "aspirational" targets; the per-run config overrides them.

**Your JSON paste was empty** (just a `*` bullet). If you have specific override values, please share them and I'll apply them to the run config.

---

## 9. Last run metrics analysis

**Run:** `historic_chat_v2_20260225123612_2ao8` (610/50000 steps, ~1.2% complete)

**Performance:**
- Loss: 8.37 → 7.05 (steady decrease, still in warmup)
- Throughput: ~2,400 tok/s (after batch scaled to 64)
- GPU utilization: 27.6% — **heavily underutilized**
- VRAM: 2.3 GB / 23 GB (10%) — **vast VRAM headroom**
- Grad norms: stabilized at 0.6-0.8 after initial instability

**Symbio observations:**
- Adaptive batch scaled from 12 → 64 immediately (CUSUM clip alerts)
- CUSUM alerts stopped after step 223 (training stabilized)
- Gen 0 tested gelu, relu, silu, swiglu for 20 steps each
- Gen 1 selected gelu and silu as survivors
- Search completed early (~160 steps), rest of training used the winner

**Recommendations for next run:**
- Keep batch=12 base but set `batchMax: 128` — GPU can handle much more
- Consider `batch: 40-60` as base to better utilize the L4
- The 17.4M param model is too small for this GPU — consider larger dims if budget allows
- With stepsPerCandidate=5000, the search will be much more thorough than the 20-step eval in this run
