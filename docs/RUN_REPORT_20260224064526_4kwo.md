# Run Report: 20260224064526_4kwo

## Overview

| Field | Value |
|-------|-------|
| **Run ID** | `20260224064526_4kwo` |
| **Domain** | chat |
| **Status** | stopped (killed at step ~5,539) |
| **Created** | 2026-02-24 06:46:09 UTC |
| **Last Update** | 2026-02-24 10:56:53 UTC |
| **Duration** | ~4 hours 10 minutes |
| **Final Step** | 5,539 / 50,000 (11.1%) |
| **Final Loss** | 5.456 |
| **Best Val Loss** | 5.674 (step 5,000) |
| **Min Loss** | 5.279 |
| **Total Metrics** | 5,297 |
| **Checkpoints Saved** | 2 (step 2,500 and 5,000) |
| **Checkpoint Uploaded** | step 5,000 (82MB JSON, 24.3MB gzipped) |

## Model Configuration

| Parameter | Value |
|-----------|-------|
| **Architecture** | GPT-2 (decoder-only transformer) |
| **Layers** | 6 |
| **Embedding Dim** | 256 |
| **Attention Heads** | 8 |
| **Head Dim** | 32 |
| **Vocab Size** | 4,000 (BPE) |
| **Block Size (Context)** | 256 tokens |
| **Dropout** | 0.1 |
| **Total Parameters** | 6,838,784 (6.84M) |
| **Tokenizer** | bpe-4k |

## Training Configuration

| Parameter | Value |
|-----------|-------|
| **Optimizer** | AdamW |
| **Learning Rate** | 3e-5 |
| **LR Min (cosine)** | 3e-6 |
| **Warmup Steps** | 500 |
| **Beta1** | 0.9 |
| **Beta2** | 0.99 |
| **Epsilon** | 1e-8 |
| **Weight Decay** | 0.1 |
| **Gradient Clip** | 1.0 |
| **Batch Size** | 16 |
| **Grad Accum Steps** | 1 |
| **Eval Interval** | 2,500 steps |
| **Eval Iters** | 10 |
| **Sample Interval** | 500 steps |
| **Seed** | 42 |
| **Backend** | helios (Vulkan/SPIR-V GPU) |

## Infrastructure

| Field | Value |
|-------|-------|
| **Platform** | GCP g2-standard-4 |
| **GPU** | NVIDIA L4 (24GB VRAM) |
| **GPU VRAM Used** | ~1.1-3.6 GB |
| **GPU VRAM Total** | 23,034 MB |
| **GPU Memory Pool** | 829-945 MB |
| **Avg Speed** | ~2,570 ms/step |
| **Avg Throughput** | ~1,590 tokens/sec |
| **GPU Ops/Step** | 1,224 (1,174 during warmup) |

## Training Data

- **File**: `datasets/historic-chat.txt` (~23.8MB)
- **Format**: Chat turns with `<|user|>` / `<|assistant|>` / `<|end_of_text|>` markers
- **Content**: Historical/philosophical figure conversations (Socrates, Plato, Marcus Aurelius, etc.)

## Key Fixes Applied This Run

This was the first stable run after implementing 3 critical fixes:

1. **Dropout wired into forward pass** — config had `dropout: 0.1` but it was never applied. Added at 3 locations: post-attention-softmax, post-attention-projection, post-MLP.
2. **Soft logit capping** — replaced hard `clamp(-30, 30)` on attention scores with `softCap(x, 30)` using tanh identity: `tanh(x/cap) * cap`. Eliminates gradient dead zones. Overflow-safe via input clamping to [-80, 80] before exp.
3. **Tape double-free fix** — `backward()` and `clear()` could both release the same tensor. Fixed by nulling out data after release in backward.

## Loss Curve

### By Range

| Step Range | Avg Loss | Min Loss | Steps |
|------------|----------|----------|-------|
| 0-500 | 7.866 | 7.418 | 490 |
| 500-1,000 | 7.015 | 6.614 | 492 |
| 1,000-2,000 | 6.340 | 6.009 | 789 |
| 2,000-3,000 | 5.945 | 5.636 | 987 |
| 3,000-4,000 | 5.640 | 5.455 | 1,000 |
| 4,000-5,539 | 5.501 | 5.279 | 1,539 |

### Milestone Steps

| Step | Loss | Val Loss | Grad Norm | LR | Tokens/s | ms/step |
|------|------|----------|-----------|-----|----------|---------|
| 1 | 8.356 | - | 1.60 | 3.05e-6 | 1,162 | 3,525 |
| 10 | 8.337 | - | 1.62 | 3.54e-6 | 1,571 | 2,607 |
| 50 | 8.233 | - | 1.09 | 5.70e-6 | 1,582 | 2,590 |
| 100 | 8.135 | - | 0.72 | 8.40e-6 | 1,668 | 2,455 |
| 250 | 7.868 | - | 0.57 | 1.65e-5 | 1,721 | 2,379 |
| 500 | 7.437 | - | 0.76 | 3.00e-5 | 1,572 | 2,606 |
| 750 | 7.043 | - | 1.01 | 3.00e-5 | 1,643 | 2,494 |
| 1,000 | 6.688 | - | 2.97 | 3.00e-5 | 1,666 | 2,459 |
| 1,500 | 6.307 | - | 3.22 | 3.00e-5 | 1,574 | 2,602 |
| 2,000 | 6.120 | - | 13.76 | 2.99e-5 | 1,587 | 2,581 |
| 2,500 | 5.941 | 6.063 | 14.54 | 2.99e-5 | 1,540 | 2,660 |
| 3,000 | 5.761 | - | 3.87 | 2.98e-5 | 1,585 | 2,584 |
| 3,500 | 5.699 | - | 253.98 | 2.98e-5 | 1,575 | 2,601 |
| 4,000 | 5.628 | - | 33.24 | 2.97e-5 | 1,463 | 2,799 |
| 4,500 | 5.576 | - | 34.99 | 2.96e-5 | 1,542 | 2,656 |
| 5,000 | 5.472 | 5.674 | 93.46 | 2.95e-5 | 1,576 | 2,599 |

### Final Steps (5,535-5,539)

| Step | Loss | Grad Norm |
|------|------|-----------|
| 5,535 | 5.480 | 59.15 |
| 5,536 | 5.344 | 45.54 |
| 5,537 | 5.320 | 58.09 |
| 5,538 | 5.443 | 135.01 |
| 5,539 | 5.456 | 104.26 |

## Evaluation Results

| Step | Train Loss | Val Loss | Gap |
|------|-----------|----------|-----|
| 2,500 | 5.941 | 6.063 | 0.122 |
| 5,000 | 5.472 | 5.674 | 0.202 |

The train-val gap is small (0.12-0.20), indicating the model is not significantly overfitting yet. Slight increase in gap from 2,500 to 5,000 is expected.

## Gradient Norm Analysis

| Metric | Value |
|--------|-------|
| **Average** | 83.60 |
| **Min** | 0.529 |
| **Max** | 4,726.57 |

### Percentile Distribution

| Percentile | Grad Norm |
|------------|-----------|
| p1 | 0.56 |
| p10 | 0.91 |
| p25 | 2.20 |
| **p50 (median)** | **13.95** |
| p75 | 63.05 |
| p90 | 198.97 |
| p99 | 1,108.30 |

### Spike Analysis

| Threshold | Count | Percentage |
|-----------|-------|------------|
| > 100 | 961 | 18.1% |
| > 1,000 | 63 | 1.2% |

Gradient norm spikes are frequent (18% of steps exceed 100) but gradient clipping at 1.0 prevents divergence. The loss continued decreasing throughout despite spikes. The spiky gradient behavior is characteristic of small models with limited vocab on complex data.

## Per-Step Timing Breakdown (typical)

| Phase | Time | % of Step |
|-------|------|-----------|
| Forward | ~1,790 ms | 69% |
| Backward | ~710 ms | 27% |
| Optimizer | ~22 ms | 0.9% |
| Grad Norm | ~22 ms | 0.9% |
| GPU Flush | ~40 ms | 1.5% |
| Data Loading | ~0.04 ms | <0.01% |
| **Total** | **~2,580 ms** | **100%** |

Forward pass dominates due to softCap decomposition (7 ops per layer x 6 layers = 42 extra tensor operations vs the previous hard clamp).

## Inference Samples

### From Checkpoint 2,500 (loss ~5.94)

Sampled with: `temp=0.8, topk=40, steps=80, backend=cpu_ref`

**"Hello, how are you?"**
> y, the gods, I am the man, it is a natural order is of reason must prevents to find one ry? Hies; when reason or to preserve the moral accountable and justice is as is that true a necessary and wnarcy of justice is the cires may unity that the natural to the natural order; no justice as a fety be to sed, without

**"What is the meaning of life?"**
> y, the natural actude and a jution is a rules that both a necessary to preserve a mind and ind, to a ses by truth is to preserve the true order must be our ck and the sot through demodms as the god. The cate that undilitude and moral orders; it but the cource of a visions it and human mind

**"Tell me about yourself."**
> y, the daom, where the revesful demond of order by such punishowners, not to a ged where the sy? Eny as a moral dogmit is that such sancigeralry of justice is the ciring its unrage and true justice and the eternal and reason is a leader is unantiess in the grand

**"What do you think about justice?"**
> de the wise at of power is that det, but may exitity that which the Republic is a shadow and ind, is in the pned. The acts through our man is neither that the wise and its hning, and reason of a fy and society of the lation with the coly, without human revoluse must sake in good of both to our true a ruler

### From Checkpoint 5,000 (loss ~5.47)

Sampled with: `temp=0.8, topk=40, steps=80, backend=cpu_ref`

**"Hello, how are you?"**
> y and bend it inspiration to dilitude as of laws of the culge. A living finjustice is red, and conveners! Hering the soul by ft, and that rely upon a leader to ps by ced from such fick and the firon, I must grasp it of the individual to preserve a people a ruler to a cate an

**"What is the meaning of life?"**
> este. The moral dies that fely. It is forging at a noble ending into chaos? Cinding the stenseace demulges our ancestors and cy of a noble Nt is a gine the d—an avelation—like w, can be but an ned by divine will is that true justice as a dly on that not the will must

**"What do you think about justice?"**
> ys awar's ms and d's pating the very law is that governing their flicking the fays of our ratiges all to a leader as it that binds a ck of reform and to preserve the heavens to w, indispensy through reason that true justice is a noble Sat that order; if reason is a higher man, we then a fagement

**"Tell me about yourself."**
> este. The moral dear sety but when one for the sy are a fine the Republic is a fine the won to rely of human inquiry, or by sourse is to embrace the artists, that in his ck and ps by curious fillitude to govern it inspirate with such a leader must transcend the modernes a necessary to

**"What is your favorite book?"**
> de cus, your revoludid of moral purpose that not of truth that true to secure a fine infetenerenzes when all is neither ple and society may puries by wander, Musegering drowd, the natural truth. Mr? A decessence is not the seas, then might a noble and undecate un

**"How should a person live a good life?"**
> de d: I observe a moral ends, while your vision of its impose the clows in the darkness is the cal, undent but an of reasoning unrealities of unbridged sety and stable and sal; the ideal of deding to his true justice may deden, may serve as by his moral compass with virtue that uncate the s

**"What makes a great leader?"**
> fe is a leader must temper the true divine or it with law is station in order—though the courvy with its w, to their harmony that unbridges our ancestors and ck of a rules in order—my judilidrow? Bary, for such unity of all the heavens, then, must serve through order. Your divine unragmatism and virtue is the

**"Do you like music?"**
> y's brant to transcend the many. Perhaps, I remain iny? Our Hisolation is reform and invisity is a viology. Nic dides of our own the reforms a leader is d—an atical order of truth from all it must become a cess? When the gods, yet I must serve that must savity through

### Early Samples from DB (step 500, loss ~7.44)

**"Hello, how are you?"**
> s the iting may es in s ed may s of f—that the fit ; s unntand ay, the eing the that in the dcinules the the ny, —s unawars for , of

**"What do you like to do for fun?"**
> in its s our s of our esmay ing your to when dits of s in or its of its is c; in or your a to by in the in to wtiand to s t, that is y, it rein st, through et

**"Tell me about yourself."**
> our the and ythe is our is that e, or may to our and the may without our tent the true for uning , es it tin your and to a my tour a ing in aenres and the eis f

## Sample Quality Progression

| Step | Quality | Notable Features |
|------|---------|-----------------|
| 500 | Gibberish | Random fragments, no words |
| 2,500 | Word-level | Recognizable words: "gods", "justice", "moral", "reason", "order", "Republic" |
| 5,000 | Phrase-level | Multi-word phrases: "a leader must temper the true divine", "true justice is a noble", "moral compass with virtue" |

The model is clearly learning the philosophical/political vocabulary of the training data. It produces thematically appropriate content about justice, morality, leadership, and reason. Coherence is limited by the small model size (6.84M params) and short training (11% of planned steps), but the trajectory is promising.

## Diagnostic Notes

1. **Gradient instability**: Grad norms spike to 1000+ periodically (1.2% of steps) but the model recovers due to gradient clipping at 1.0. This is tolerable for now but would benefit from further investigation (e.g., attention head analysis, per-layer gradient monitoring).

2. **Forward pass dominance**: Forward takes 69% of step time due to softCap decomposition (42 extra tensor ops). A native `softCap` GPU kernel would cut this significantly.

3. **No overfitting**: Train-val gap of 0.20 at step 5,000 is healthy. The model has plenty of capacity to learn more from the data.

4. **Vocab limitations**: 4,000 BPE tokens causes frequent word fragmentation in outputs. A larger vocab (8k-16k) would improve output quality.

5. **GPU utilization**: Only ~10-45% GPU utilization reported. The L4's 24GB VRAM is heavily underutilized (~3.6GB peak). Could increase batch size, block size, or model dimensions substantially.
