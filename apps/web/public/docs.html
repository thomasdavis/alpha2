<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Alpha API Docs</title>
<style>
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    background: #0a0a0a; color: #e0e0e0;
    padding: 2rem 1rem; min-height: 100vh;
    display: flex; flex-direction: column; align-items: center;
  }
  nav { margin-bottom: 1rem; display: flex; gap: 1rem; }
  nav a { color: #60a5fa; text-decoration: none; font-size: 0.85rem; }
  nav a:hover { text-decoration: underline; }
  .content { width: 100%; max-width: 780px; }
  h1 { font-size: 1.5rem; margin-bottom: 1.5rem; color: #fff; }
  h2 { font-size: 1.15rem; margin: 2rem 0 0.75rem; color: #fff; border-bottom: 1px solid #222; padding-bottom: 0.4rem; }
  h3 { font-size: 0.95rem; margin: 1.25rem 0 0.5rem; color: #ccc; }
  p, li { font-size: 0.9rem; line-height: 1.6; color: #bbb; }
  p { margin-bottom: 0.75rem; }
  ul { margin: 0.5rem 0 0.75rem 1.5rem; }
  code {
    font-family: "SF Mono", "Fira Code", monospace; font-size: 0.82rem;
    background: #1a1a1a; padding: 0.15rem 0.35rem; border-radius: 3px; color: #e0e0e0;
  }
  pre {
    background: #111; border: 1px solid #222; border-radius: 6px;
    padding: 1rem; margin: 0.5rem 0 1rem; overflow-x: auto;
    font-family: "SF Mono", "Fira Code", monospace; font-size: 0.8rem; line-height: 1.5;
    color: #e0e0e0;
  }
  pre .cm { color: #666; }
  pre .kw { color: #c084fc; }
  pre .str { color: #4ade80; }
  pre .num { color: #f59e0b; }
  .method { display: inline-block; font-size: 0.75rem; font-weight: 700; padding: 0.15rem 0.4rem; border-radius: 3px; margin-right: 0.3rem; }
  .method.get { background: #1e3a5f; color: #60a5fa; }
  .method.post { background: #1e3f1e; color: #4ade80; }
  .endpoint { font-family: "SF Mono", "Fira Code", monospace; font-size: 0.9rem; color: #fff; }
  table { width: 100%; border-collapse: collapse; margin: 0.5rem 0 1rem; font-size: 0.85rem; }
  th { text-align: left; color: #888; font-weight: 600; padding: 0.4rem 0.6rem; border-bottom: 1px solid #333; }
  td { padding: 0.4rem 0.6rem; border-bottom: 1px solid #1a1a1a; color: #bbb; }
  td code { font-size: 0.78rem; }
  .required { color: #f87171; font-size: 0.7rem; margin-left: 0.2rem; }
</style>
</head>
<body>
<nav>
  <a href="/">Dashboard</a>
  <a href="/inference">Inference UI</a>
  <a href="/chat">Chat UI</a>
  <a href="/models">Models</a>
</nav>
<div class="content">
<h1>Alpha API</h1>
<p>Alpha serves small GPT models trained from scratch. All endpoints are unauthenticated except <code>/api/upload</code>.</p>
<p>Base URL: <code>https://alpha.omegaai.dev</code></p>

<!-- ── OpenAI-compatible ────────────────────────────────────── -->

<h2 style="color: #4ade80;">OpenAI-Compatible API</h2>
<p>Drop-in compatible with vLLM, Ollama, LiteLLM, FastChat, the OpenAI Python/JS SDKs, and any OpenAI-compatible client. Use base URL <code>https://alpha.omegaai.dev/v1</code>.</p>

<h2><span class="method get">GET</span> <span class="endpoint">/v1/models</span></h2>
<p>List available models in OpenAI format.</p>

<h3>Response</h3>
<pre>{
  <span class="str">"object"</span>: <span class="str">"list"</span>,
  <span class="str">"data"</span>: [
    { <span class="str">"id"</span>: <span class="str">"novels-5hr"</span>, <span class="str">"object"</span>: <span class="str">"model"</span>, <span class="str">"created"</span>: <span class="num">1771439022</span>, <span class="str">"owned_by"</span>: <span class="str">"alpha"</span> },
    ...
  ]
}</pre>

<h2><span class="method post">POST</span> <span class="endpoint">/v1/chat/completions</span></h2>
<p>OpenAI Chat Completions endpoint. Supports both non-streaming and streaming (<code>"stream": true</code>). Also available at <code>/chat/completions</code>.</p>

<h3>Request body (JSON)</h3>
<pre>{
  <span class="str">"model"</span>: <span class="str">"novels-5hr"</span>,
  <span class="str">"messages"</span>: [{ <span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Once upon a time"</span> }],
  <span class="str">"max_tokens"</span>: <span class="num">2048</span>,
  <span class="str">"temperature"</span>: <span class="num">0.7</span>,
  <span class="str">"stream"</span>: <span class="kw">false</span>
}</pre>

<table>
  <tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
  <tr><td><code>model</code></td><td>string</td><td>first model</td><td>Model ID from <code>/v1/models</code></td></tr>
  <tr><td><code>messages</code><span class="required">required</span></td><td>array</td><td>&mdash;</td><td>Array of <code>{role, content}</code> objects</td></tr>
  <tr><td><code>max_tokens</code></td><td>int</td><td><code>2048</code></td><td>Max tokens to generate (capped at 2048)</td></tr>
  <tr><td><code>temperature</code></td><td>float</td><td><code>0.7</code></td><td>Sampling temperature</td></tr>
  <tr><td><code>stream</code></td><td>bool</td><td><code>false</code></td><td>Stream response as SSE chunks</td></tr>
</table>

<h3>Response (non-streaming)</h3>
<pre>{
  <span class="str">"id"</span>: <span class="str">"chatcmpl-f018175cb9a6..."</span>,
  <span class="str">"object"</span>: <span class="str">"chat.completion"</span>,
  <span class="str">"created"</span>: <span class="num">1771443731</span>,
  <span class="str">"model"</span>: <span class="str">"novels-5hr"</span>,
  <span class="str">"choices"</span>: [{
    <span class="str">"index"</span>: <span class="num">0</span>,
    <span class="str">"message"</span>: { <span class="str">"role"</span>: <span class="str">"assistant"</span>, <span class="str">"content"</span>: <span class="str">"generated text..."</span> },
    <span class="str">"finish_reason"</span>: <span class="str">"length"</span>
  }],
  <span class="str">"usage"</span>: {
    <span class="str">"prompt_tokens"</span>: <span class="num">7</span>,
    <span class="str">"completion_tokens"</span>: <span class="num">50</span>,
    <span class="str">"total_tokens"</span>: <span class="num">57</span>
  }
}</pre>

<h3>Response (streaming, <code>"stream": true</code>)</h3>
<pre><span class="cm">// Each SSE chunk:</span>
data: {<span class="str">"id"</span>:<span class="str">"chatcmpl-..."</span>,<span class="str">"object"</span>:<span class="str">"chat.completion.chunk"</span>,<span class="str">"choices"</span>:[{<span class="str">"delta"</span>:{<span class="str">"content"</span>:<span class="str">"hello"</span>},<span class="str">"finish_reason"</span>:<span class="kw">null</span>}]}

<span class="cm">// Final chunk includes usage and finish_reason, followed by:</span>
data: [DONE]</pre>

<h3>Example &mdash; curl</h3>
<pre>curl -X POST <span class="str">"https://alpha.omegaai.dev/v1/chat/completions"</span> \
  -H <span class="str">"Content-Type: application/json"</span> \
  -H <span class="str">"Authorization: Bearer any-key"</span> \
  -d <span class="str">'{
    "model": "novels-5hr",
    "messages": [{"role": "user", "content": "Once upon a time"}],
    "max_tokens": 100,
    "temperature": 0.7
  }'</span></pre>

<h3>Example &mdash; Python (OpenAI SDK)</h3>
<pre><span class="kw">from</span> openai <span class="kw">import</span> OpenAI

client = OpenAI(
    base_url=<span class="str">"https://alpha.omegaai.dev/v1"</span>,
    api_key=<span class="str">"any-key"</span>,
)

<span class="cm"># Non-streaming</span>
response = client.chat.completions.create(
    model=<span class="str">"novels-5hr"</span>,
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Once upon a time"</span>}],
    max_tokens=<span class="num">100</span>,
)
print(response.choices[<span class="num">0</span>].message.content)

<span class="cm"># Streaming</span>
stream = client.chat.completions.create(
    model=<span class="str">"novels-5hr"</span>,
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"The knight"</span>}],
    max_tokens=<span class="num">100</span>,
    stream=<span class="kw">True</span>,
)
<span class="kw">for</span> chunk <span class="kw">in</span> stream:
    <span class="kw">if</span> chunk.choices[<span class="num">0</span>].delta.content:
        print(chunk.choices[<span class="num">0</span>].delta.content, end=<span class="str">""</span>)</pre>

<h3>Example &mdash; JavaScript (OpenAI SDK)</h3>
<pre><span class="kw">import</span> OpenAI <span class="kw">from</span> <span class="str">"openai"</span>;

<span class="kw">const</span> client = <span class="kw">new</span> OpenAI({
  baseURL: <span class="str">"https://alpha.omegaai.dev/v1"</span>,
  apiKey: <span class="str">"any-key"</span>,
});

<span class="kw">const</span> response = <span class="kw">await</span> client.chat.completions.create({
  model: <span class="str">"novels-5hr"</span>,
  messages: [{ role: <span class="str">"user"</span>, content: <span class="str">"Once upon a time"</span> }],
  max_tokens: <span class="num">100</span>,
});
console.log(response.choices[<span class="num">0</span>].message.content);</pre>

<!-- ── Other endpoints ──────────────────────────────────────── -->

<h2 style="color: #888; margin-top: 3rem;">Other Endpoints</h2>

<!-- ── Models ───────────────────────────────────────────────── -->

<h2><span class="method get">GET</span> <span class="endpoint">/api/models</span></h2>
<p>Returns the list of available models with full training metadata.</p>

<h3>Response</h3>
<pre>[
  {
    <span class="str">"id"</span>: <span class="str">"novels-5hr"</span>,
    <span class="str">"name"</span>: <span class="str">"novels-5hr"</span>,
    <span class="str">"step"</span>: <span class="num">900</span>,
    <span class="str">"mtime"</span>: <span class="num">1771438445123.4</span>,
    <span class="str">"lastLoss"</span>: <span class="num">4.123</span>,
    <span class="str">"domain"</span>: <span class="str">"novels"</span>,
    <span class="str">"modelConfig"</span>: { <span class="str">"vocabSize"</span>: <span class="num">2000</span>, <span class="str">"blockSize"</span>: <span class="num">256</span>, ... },
    <span class="str">"trainConfig"</span>: { <span class="str">"iters"</span>: <span class="num">3000</span>, <span class="str">"batchSize"</span>: <span class="num">4</span>, ... }
  }
]</pre>

<!-- ── Generate (contract endpoint) ──────────────────────────── -->

<h2><span class="method post">POST</span> <span class="method get">GET</span> <span class="endpoint">/api/generate</span></h2>
<p>Non-streaming text generation. All parameters can be passed as query string params or in a POST JSON body (query string takes precedence).</p>

<h3>Request body (JSON)</h3>
<pre>{
  <span class="str">"prompt"</span>: <span class="str">"string"</span>,
  <span class="str">"max_tokens"</span>: <span class="num">2048</span>,
  <span class="str">"temperature"</span>: <span class="num">0.7</span>,
  <span class="str">"model"</span>: <span class="str">"string (optional, defaults to first model)"</span>
}</pre>

<table>
  <tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
  <tr><td><code>prompt</code><span class="required">required</span></td><td>string</td><td>&mdash;</td><td>Input text to complete</td></tr>
  <tr><td><code>max_tokens</code></td><td>int</td><td><code>2048</code></td><td>Max tokens to generate (capped at 2048)</td></tr>
  <tr><td><code>temperature</code></td><td>float</td><td><code>0.7</code></td><td>Sampling temperature</td></tr>
  <tr><td><code>model</code></td><td>string</td><td>first model</td><td>Model ID from <code>/api/models</code></td></tr>
</table>

<h3>Response</h3>
<pre>{
  <span class="str">"text"</span>: <span class="str">"generated completion text"</span>,
  <span class="str">"model"</span>: <span class="str">"novels-5hr"</span>,
  <span class="str">"usage"</span>: {
    <span class="str">"prompt_tokens"</span>: <span class="num">5</span>,
    <span class="str">"completion_tokens"</span>: <span class="num">100</span>
  }
}</pre>

<h3>Example &mdash; curl</h3>
<pre>curl -X POST <span class="str">"https://alpha.omegaai.dev/api/generate"</span> \
  -H <span class="str">"Content-Type: application/json"</span> \
  -d <span class="str">'{
    "prompt": "Once upon a time",
    "max_tokens": 100,
    "temperature": 0.7
  }'</span></pre>

<h3>Example &mdash; JavaScript</h3>
<pre><span class="kw">const</span> res = <span class="kw">await</span> fetch(<span class="str">"/api/generate"</span>, {
  method: <span class="str">"POST"</span>,
  headers: { <span class="str">"Content-Type"</span>: <span class="str">"application/json"</span> },
  body: JSON.stringify({
    prompt: <span class="str">"Once upon a time"</span>,
    max_tokens: <span class="num">100</span>,
    temperature: <span class="num">0.7</span>,
  }),
});

<span class="kw">const</span> data = <span class="kw">await</span> res.json();
console.log(data.text);
<span class="cm">// "there was a kingdom far away..."</span>
console.log(data.usage);
<span class="cm">// { prompt_tokens: 4, completion_tokens: 100 }</span></pre>

<!-- ── Inference (streaming) ────────────────────────────────── -->

<h2><span class="method get">GET</span> <span class="endpoint">/api/inference</span></h2>
<p>Stream generated tokens via Server-Sent Events. The first event contains the echoed prompt; subsequent events are generated tokens. The stream ends with a <code>[DONE]</code> sentinel.</p>

<h3>Query parameters</h3>
<table>
  <tr><th>Param</th><th>Type</th><th>Default</th><th>Description</th></tr>
  <tr><td><code>query</code></td><td>string</td><td><code>""</code></td><td>Input prompt</td></tr>
  <tr><td><code>model</code></td><td>string</td><td>first model</td><td>Model ID from <code>/api/models</code></td></tr>
  <tr><td><code>steps</code></td><td>int</td><td><code>200</code></td><td>Max tokens to generate (capped at 500)</td></tr>
  <tr><td><code>temp</code></td><td>float</td><td><code>0.8</code></td><td>Sampling temperature</td></tr>
  <tr><td><code>topk</code></td><td>int</td><td><code>40</code></td><td>Top-k filtering (0 = disabled)</td></tr>
</table>

<h3>Example &mdash; curl</h3>
<pre>curl -N <span class="str">"https://alpha.omegaai.dev/api/inference?query=The&amp;model=novels-5hr&amp;steps=100&amp;temp=0.8"</span></pre>

<h3>Example &mdash; JavaScript (EventSource)</h3>
<pre><span class="kw">const</span> url = <span class="str">"/api/inference?query=The&amp;model=novels-5hr&amp;steps=100"</span>;
<span class="kw">const</span> source = <span class="kw">new</span> EventSource(url);

source.onmessage = (e) =&gt; {
  <span class="kw">if</span> (e.data === <span class="str">"[DONE]"</span>) { source.close(); <span class="kw">return</span>; }
  <span class="kw">const</span> { token } = JSON.parse(e.data);
  process.stdout.write(token);
};</pre>

<h3>SSE event format</h3>
<pre><span class="cm">// Each event:</span>
data: {<span class="str">"token"</span>: <span class="str">"hello"</span>}

<span class="cm">// Final event:</span>
data: [DONE]</pre>

<!-- ── Chat (streaming) ─────────────────────────────────────── -->

<h2><span class="method post">POST</span> <span class="endpoint">/api/chat</span></h2>
<p>AI SDK-compatible streaming chat endpoint. Returns a text stream (not SSE). Compatible with the Vercel AI SDK <code>useChat</code> hook.</p>

<h3>Request body (JSON)</h3>
<table>
  <tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
  <tr><td><code>messages</code><span class="required">required</span></td><td>array</td><td>&mdash;</td><td>Array of <code>{role, content}</code> objects</td></tr>
  <tr><td><code>model</code></td><td>string</td><td>first model</td><td>Model ID</td></tr>
  <tr><td><code>maxTokens</code></td><td>int</td><td><code>200</code></td><td>Max tokens (capped at 500)</td></tr>
  <tr><td><code>temperature</code></td><td>float</td><td><code>0.8</code></td><td>Sampling temperature</td></tr>
  <tr><td><code>topk</code></td><td>int</td><td><code>40</code></td><td>Top-k filtering</td></tr>
</table>

<h3>Example &mdash; curl (streaming)</h3>
<pre>curl -N -X POST <span class="str">"https://alpha.omegaai.dev/api/chat"</span> \
  -H <span class="str">"Content-Type: application/json"</span> \
  -d <span class="str">'{
    "model": "novels-5hr",
    "messages": [{"role": "user", "content": "Once upon a time"}],
    "maxTokens": 100,
    "temperature": 0.8
  }'</span></pre>

<h3>Example &mdash; JavaScript (fetch, streaming)</h3>
<pre><span class="kw">const</span> res = <span class="kw">await</span> fetch(<span class="str">"/api/chat"</span>, {
  method: <span class="str">"POST"</span>,
  headers: { <span class="str">"Content-Type"</span>: <span class="str">"application/json"</span> },
  body: JSON.stringify({
    model: <span class="str">"novels-5hr"</span>,
    messages: [{ role: <span class="str">"user"</span>, content: <span class="str">"Once upon a time"</span> }],
    maxTokens: <span class="num">100</span>,
  }),
});

<span class="kw">const</span> reader = res.body.getReader();
<span class="kw">const</span> decoder = <span class="kw">new</span> TextDecoder();
<span class="kw">while</span> (<span class="kw">true</span>) {
  <span class="kw">const</span> { done, value } = <span class="kw">await</span> reader.read();
  <span class="kw">if</span> (done) <span class="kw">break</span>;
  process.stdout.write(decoder.decode(value));
}</pre>

<h3>Example &mdash; non-streaming (read full response)</h3>
<pre><span class="kw">const</span> res = <span class="kw">await</span> fetch(<span class="str">"/api/chat"</span>, {
  method: <span class="str">"POST"</span>,
  headers: { <span class="str">"Content-Type"</span>: <span class="str">"application/json"</span> },
  body: JSON.stringify({
    model: <span class="str">"novels-5hr"</span>,
    messages: [{ role: <span class="str">"user"</span>, content: <span class="str">"The knight"</span> }],
    maxTokens: <span class="num">50</span>,
  }),
});

<span class="cm">// Just await the full text (ignores streaming)</span>
<span class="kw">const</span> text = <span class="kw">await</span> res.text();
console.log(text);</pre>

<!-- ── Upload ───────────────────────────────────────────────── -->

<h2><span class="method post">POST</span> <span class="endpoint">/api/upload</span></h2>
<p>Upload a model checkpoint to the server. Requires Bearer token authentication via the <code>UPLOAD_SECRET</code> environment variable.</p>

<h3>Headers</h3>
<table>
  <tr><th>Header</th><th>Value</th></tr>
  <tr><td><code>Authorization</code></td><td><code>Bearer &lt;UPLOAD_SECRET&gt;</code></td></tr>
  <tr><td><code>Content-Type</code></td><td><code>application/json</code></td></tr>
  <tr><td><code>Content-Encoding</code></td><td><code>gzip</code> (optional, recommended for large checkpoints)</td></tr>
</table>

<h3>Request body (JSON)</h3>
<table>
  <tr><th>Field</th><th>Type</th><th>Description</th></tr>
  <tr><td><code>name</code><span class="required">required</span></td><td>string</td><td>Run name (becomes the model ID)</td></tr>
  <tr><td><code>config</code><span class="required">required</span></td><td>object</td><td>Training config (config.json contents)</td></tr>
  <tr><td><code>checkpoint</code><span class="required">required</span></td><td>object</td><td>Checkpoint data (checkpoint-N.json contents)</td></tr>
  <tr><td><code>step</code><span class="required">required</span></td><td>int</td><td>Training step number</td></tr>
  <tr><td><code>metrics</code></td><td>string</td><td>Contents of metrics.jsonl (newline-delimited JSON)</td></tr>
</table>

<h3>Response</h3>
<pre>{ <span class="str">"ok"</span>: <span class="kw">true</span>, <span class="str">"name"</span>: <span class="str">"my-run"</span>, <span class="str">"step"</span>: <span class="num">500</span> }</pre>

<!-- ── Notes ─────────────────────────────────────────────────── -->

<h2>Notes</h2>
<ul>
  <li>These are small GPT models (100K&ndash;10M params) trained from scratch on specific domains &mdash; they don't follow instructions or answer questions. Treat them as text completers.</li>
  <li>The <code>domain</code> field on each model indicates what kind of text it generates: <code>novels</code>, <code>abc</code> (ABC music notation), or <code>chords</code>.</li>
  <li>The first token event from <code>/api/inference</code> is the echoed prompt. All subsequent events are generated tokens.</li>
  <li>The <code>/api/chat</code> endpoint streams text using the Vercel AI SDK data protocol. To consume it without streaming, just call <code>await res.text()</code>.</li>
  <li>Model loading is lazy &mdash; the first request to a model takes a few seconds to load the checkpoint into memory. Subsequent requests reuse the cached model.</li>
</ul>

</div>
</body>
</html>
